{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMRCU/+TLOjis8IcmPoVqLO",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MichalMiszcz/ai_playing_music/blob/main/LSTM_autoenkoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Biblioteki"
   ],
   "metadata": {
    "id": "vqx4TNjopsAH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "id": "kvpV6vv3ptTi",
    "ExecuteTime": {
     "end_time": "2026-02-23T13:13:24.118440Z",
     "start_time": "2026-02-23T13:13:19.602842Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generowanie przykÅ‚adowych danych"
   ],
   "metadata": {
    "id": "rBMM3CQDoaGZ"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bm8Ame0Snqse",
    "ExecuteTime": {
     "end_time": "2026-02-23T15:32:17.230640Z",
     "start_time": "2026-02-23T15:32:17.184174Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "random_notes = random.choices(range(1, 9), k=10)\n",
    "random_times = random.choices(range(1000, 10000), k=10)\n",
    "\n",
    "sequences = []\n",
    "\n",
    "for n in range(512):\n",
    "  lstm_sequence = [[n, dt] for n in random_notes for dt in random_times]\n",
    "  sequences.append(lstm_sequence)\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Architektura sieci"
   ],
   "metadata": {
    "id": "Hkqj2igfpRrs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class AutoenkoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, input_dim=2, output_dim=2, rnn_layers=2, max_seq_len=100) -> None:\n",
    "        super(AutoenkoderLSTM, self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.encoder = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_layers, dropout=0.33, batch_first=True)\n",
    "        self.decoder = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=rnn_layers, dropout=0.33, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        self.proj_h = nn.Linear(hidden_dim, hidden_dim * rnn_layers)\n",
    "        self.proj_c = nn.Linear(hidden_dim, hidden_dim * rnn_layers)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        features, _ = self.encoder(x)\n",
    "\n",
    "        input_seq = torch.cat([torch.zeros(batch_size, 1, self.output_dim).to(x.device), target[:, :-1, :]], dim=1)\n",
    "        h0 = self.proj_h(features).view(batch_size, self.rnn.num_layers, -1).transpose(0, 1).contiguous()\n",
    "        c0 = self.proj_c(features).view(batch_size, self.rnn.num_layers, -1).transpose(0, 1).contiguous()\n",
    "\n",
    "        output, _ = self.decoder(input_seq, (h0, c0))\n",
    "\n",
    "        self.linear.flatten_parameters()\n",
    "\n"
   ],
   "metadata": {
    "id": "yaQkuW3HpRbL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Zmienne"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "batch_size = 8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Uczenie"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T16:09:31.558475Z",
     "start_time": "2026-02-23T16:09:31.549571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "proj_h = nn.Linear(2, 16)\n",
    "x = [[0.1, 0.2]]\n",
    "x = torch.tensor(x)\n",
    "print(x)\n",
    "proj_1 = proj_h(x)\n",
    "# view_1 = x.view(2, 2, -1).transpose(0, 1).contiguous()\n",
    "proj = proj_h(x).view(2, 2, -1).transpose(0, 1).contiguous()\n",
    "\n",
    "print(proj_1)\n",
    "print(proj)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.2000]])\n",
      "tensor([[-0.0599, -0.3562,  0.1286,  0.4143,  0.0901,  0.5203,  0.4646,  0.0214,\n",
      "         -0.0503, -0.3505, -0.3407, -0.0952, -0.4608,  0.2438,  0.1360,  0.5337]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[[-0.0599, -0.3562,  0.1286,  0.4143],\n",
      "         [-0.0503, -0.3505, -0.3407, -0.0952]],\n",
      "\n",
      "        [[ 0.0901,  0.5203,  0.4646,  0.0214],\n",
      "         [-0.4608,  0.2438,  0.1360,  0.5337]]], grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "execution_count": 60
  }
 ]
}
