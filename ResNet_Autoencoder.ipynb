{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-16T14:13:27.731407Z",
          "start_time": "2026-02-16T14:13:27.710294Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydd39X5S1RYf",
        "outputId": "1fbdecf6-5dac-41e1-f38f-318a3fd5648b"
      },
      "source": [
        "import sys\n",
        "\n",
        "print(sys.executable)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p0hn_9G--nIC"
      },
      "outputs": [],
      "source": [
        "# !nproc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMfQ7vpl1RYh",
        "outputId": "c1e39066-1295-4c93-e86d-f4ab8a27c3a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-26.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-26.0.1\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-16T14:13:35.281385Z",
          "start_time": "2026-02-16T14:13:29.520627Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gk-Udze1RYh",
        "outputId": "9391e38a-f739-4b28-96e6-d2f6d52e09d8"
      },
      "source": [
        "%pip install opencv-python tqdm matplotlib"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.13.0.92)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_FlbE9cH_U8y",
        "outputId": "79cb7a8d-1b21-4010-b5a4-fbdf6bba3de8",
        "ExecuteTime": {
          "end_time": "2026-02-16T14:13:40.667319Z",
          "start_time": "2026-02-16T14:13:36.521237Z"
        }
      },
      "source": [
        "# Install dependencies\n",
        "%pip install torch numpy torchvision"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Yt9xVz_cgf",
        "ExecuteTime": {
          "end_time": "2026-02-16T14:13:51.434046Z",
          "start_time": "2026-02-16T14:13:40.758665Z"
        }
      },
      "source": [
        "# Import libraries\n",
        "# import mido\n",
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "from os import mkdir\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# from music21 import converter\n",
        "# from pdf2image import convert_from_path\n",
        "# from google.colab import files\n",
        "import shutil\n",
        "import numpy as np\n",
        "# import cv2\n",
        "import datetime"
      ],
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntqLWG0OzehP",
        "outputId": "2573f53b-aafe-4fed-edbc-22ba249dc5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'ai_playing_music': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm -r ai_playing_music"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z3CCkSfBWFz",
        "outputId": "215c3eaa-dd5f-4c04-9b26-f34685a41ad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai_playing_music'...\n",
            "remote: Enumerating objects: 95499, done.\u001b[K\n",
            "remote: Counting objects: 100% (6068/6068), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2775/2775), done.\u001b[K\n",
            "remote: Total 95499 (delta 2207), reused 6010 (delta 2154), pack-reused 89431 (from 1)\u001b[K\n",
            "Receiving objects: 100% (95499/95499), 1.61 GiB | 20.26 MiB/s, done.\n",
            "Resolving deltas: 100% (22117/22117), done.\n",
            "Updating files: 100% (46976/46976), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/MichalMiszcz/ai_playing_music.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "6vkJgFT6sBPL"
      },
      "outputs": [],
      "source": [
        "# %cd /content/ai_playing_music\n",
        "# !git pull"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfJ01Z0l-6zN",
        "ExecuteTime": {
          "end_time": "2026-02-16T14:13:59.392708Z",
          "start_time": "2026-02-16T14:13:59.386705Z"
        }
      },
      "source": [
        "global count_tracks\n",
        "count_tracks = 0"
      ],
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ6gl8DXYgER",
        "ExecuteTime": {
          "end_time": "2026-02-16T14:14:00.464825Z",
          "start_time": "2026-02-16T14:14:00.459446Z"
        }
      },
      "source": [
        "HEIGHT = 390\n",
        "WIDTH = 585\n",
        "\n",
        "WHITE_KEYS_MIDI = [60, 62, 64, 65, 67, 69, 71, 72]\n",
        "NUM_NOTES = len(WHITE_KEYS_MIDI)\n",
        "\n",
        "VELOCITY = [0, 90]\n",
        "NUM_VELOCITIES = len(VELOCITY)\n",
        "\n",
        "DELTA_TIME = [0, 5040, 10080, 20160, 30240, 40320]\n",
        "NUM_DELTA_TIME = len(DELTA_TIME)"
      ],
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhhg2yrhDKlP"
      },
      "source": [
        "# AutoEncoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73MQmCXnryF6"
      },
      "source": [
        "## Enkoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFg1K8AXrz8y",
        "ExecuteTime": {
          "end_time": "2026-02-16T14:14:08.904491Z",
          "start_time": "2026-02-16T14:14:08.875329Z"
        }
      },
      "source": [
        "from typing import Callable, List, Optional, Type\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "\"\"\"From https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html#resnet18\"\"\"\n",
        "\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlockEnc(nn.Module):\n",
        "    \"\"\"The basic block architecture of resnet-18 network.\n",
        "    \"\"\"\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"The encoder model.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[BasicBlockEnc],\n",
        "        layers: List[int],\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlockEnc) and m.bn2.weight is not None:\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def _make_layer(\n",
        "        self,\n",
        "        block: Type[BasicBlockEnc],\n",
        "        planes: int,\n",
        "        blocks: int,\n",
        "        stride: int = 1,\n",
        "        dilate: bool = False,\n",
        "    ) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)"
      ],
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLrQ7plQsCyW"
      },
      "source": [
        "## Dekoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W-7UDJdsEIm",
        "ExecuteTime": {
          "end_time": "2026-02-16T14:14:24.285896Z",
          "start_time": "2026-02-16T14:14:24.266379Z"
        }
      },
      "source": [
        "from typing import Callable, List, Optional, Type\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "\"\"\"Based on https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html#resnet18\"\"\"\n",
        "\n",
        "\n",
        "def conv3x3Transposed(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1, output_padding: int = 0) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\n",
        "    \"\"\"\n",
        "    return nn.ConvTranspose2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        output_padding = output_padding, # output_padding is neccessary to invert conv2d with stride > 1\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "def conv1x1Transposed(in_planes: int, out_planes: int, stride: int = 1, output_padding: int = 0) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\n",
        "    \"\"\"\n",
        "    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False, output_padding = output_padding)\n",
        "\n",
        "\n",
        "class BasicBlockDec(nn.Module):\n",
        "    \"\"\"The basic block architecture of resnet-18 network.\n",
        "    \"\"\"\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        output_padding: int = 0,\n",
        "        upsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3Transposed(planes, inplanes, stride, output_padding=output_padding)\n",
        "        self.bn1 = norm_layer(inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3Transposed(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.upsample = upsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "        out = self.conv2(x)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn1(out)\n",
        "\n",
        "        if self.upsample is not None:\n",
        "            identity = self.upsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"The decoder model.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[BasicBlockDec],\n",
        "        layers: List[int],\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64 # change from 2048 to 64. It should be the shape of the output image chanel.\n",
        "        self.dilation = 1\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.de_conv1 = nn.ConvTranspose2d(self.inplanes, 3, kernel_size=7, stride=2, padding=3, bias=False, output_padding=1)\n",
        "        self.bn1 = norm_layer(3)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.unpool = nn.Upsample(scale_factor=2, mode='bilinear') # NOTE: invert max pooling\n",
        "\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1 ,output_padding = 0, last_block_dim=64)\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlockDec) and m.bn2.weight is not None:\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def _make_layer(\n",
        "        self,\n",
        "        block: Type[BasicBlockDec],\n",
        "        planes: int,\n",
        "        blocks: int,\n",
        "        stride: int = 2,\n",
        "        output_padding: int = 1, # NOTE: output_padding will correct the dimensions of inverting conv2d with stride > 1.\n",
        "        # More info:https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
        "        last_block_dim: int = 0,\n",
        "    ) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        upsample = None\n",
        "        previous_dilation = self.dilation\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if last_block_dim == 0:\n",
        "            last_block_dim = self.inplanes//2\n",
        "\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            upsample = nn.Sequential(\n",
        "                conv1x1Transposed(planes * block.expansion, last_block_dim, stride, output_padding),\n",
        "                norm_layer(last_block_dim),\n",
        "            )\n",
        "\n",
        "        layers.append( block(\n",
        "                last_block_dim, planes, stride, output_padding, upsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
        "            ))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer1(x)\n",
        "\n",
        "        x = self.unpool(x)\n",
        "        x = self.de_conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)"
      ],
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc2Tw0Fzs42e"
      },
      "source": [
        "## Autoenkoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOUbsMkfs4iE",
        "ExecuteTime": {
          "end_time": "2026-02-16T14:14:30.498090Z",
          "start_time": "2026-02-16T14:14:30.489567Z"
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class AE(nn.Module):\n",
        "    \"\"\"Construction of resnet autoencoder.\n",
        "\n",
        "    Attributes:\n",
        "        network (str): the architectural type of the network. There are 2 choices:\n",
        "            - 'default' (default), related with the original resnet-18 architecture\n",
        "            - 'light', a samller network implementation of resnet-18 for smaller input images.\n",
        "        num_layers (int): the number of layers to be created. Implemented for 18 layers (default) for both types\n",
        "            of network, 34 layers for default only network and 20 layers for light network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, network='default', num_layers=18):\n",
        "        \"\"\"Initialize the autoencoder.\n",
        "\n",
        "        Args:\n",
        "            network (str): a flag to efine the network version. Choices ['default' (default), 'light'].\n",
        "             num_layers (int): the number of layers to be created. Choices [18 (default), 34 (only for\n",
        "                'default' network), 20 (only for 'light' network).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.network = network\n",
        "        if self.network == 'default':\n",
        "            if num_layers==18:\n",
        "                # resnet 18 encoder\n",
        "                self.encoder = Encoder(BasicBlockEnc, [2, 2, 2, 2])\n",
        "                # resnet 18 decoder\n",
        "                self.decoder = Decoder(BasicBlockDec, [2, 2, 2, 2])\n",
        "            elif num_layers==34:\n",
        "                # resnet 34 encoder\n",
        "                self.encoder = Encoder(BasicBlockEnc, [3, 4, 6, 3])\n",
        "                # resnet 34 decoder\n",
        "                self.decoder = Decoder(BasicBlockDec, [3, 4, 6, 3])\n",
        "            else:\n",
        "                raise NotImplementedError(\"Only resnet 18 & 34 autoencoder have been implemented for images size >= 64x64.\")\n",
        "        else:\n",
        "                raise NotImplementedError(\"Only default and light resnet have been implemented. Th light version corresponds to input datasets with size less than 64x64.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"The forward functon of the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.tensor): the batched input data\n",
        "\n",
        "        Returns:\n",
        "            x (torch.tensor): encoder result\n",
        "            z (torch.tensor): decoder result\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)\n",
        "        x = self.decoder(z)\n",
        "        return x, z\n"
      ],
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFTmvT8CDP8B"
      },
      "source": [
        "# Music Image Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gIaSgunDQPR",
        "ExecuteTime": {
          "end_time": "2026-02-16T14:14:37.237824Z",
          "start_time": "2026-02-16T14:14:33.579430Z"
        }
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "# import mido\n",
        "\n",
        "note_to_index = {midi_num: i for i, midi_num in enumerate(WHITE_KEYS_MIDI)}\n",
        "velocity_to_index = {midi_num: i for i, midi_num in enumerate(VELOCITY)}\n",
        "delta_time_to_index = {midi_num: i for i, midi_num in enumerate(DELTA_TIME)}\n",
        "\n",
        "class MusicImageDataset(Dataset):\n",
        "    def __init__(self, image_root, midi_root, image_transform=None, max_seq_len=100, max_midi_files=100, modify_image=False):\n",
        "        self.image_root = image_root\n",
        "        self.midi_root = midi_root\n",
        "        self.image_transform = image_transform if image_transform else transforms.ToTensor()\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.modify_image = modify_image\n",
        "\n",
        "        midi_files = []\n",
        "        for root, dirs, files in os.walk(midi_root):\n",
        "            folder = os.path.basename(os.path.dirname(root))\n",
        "            author = os.path.basename(root)\n",
        "            for file in files:\n",
        "                if file.endswith('.mid'):\n",
        "                    midi_files.append((folder, author, os.path.join(root, file)))\n",
        "\n",
        "        random.shuffle(midi_files)\n",
        "        sorted_midi_files = sorted(midi_files[:max_midi_files], key=lambda x: x[2])\n",
        "        self.selected_midi_files = sorted_midi_files\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.midi_features = {}\n",
        "        records_to_remove = []\n",
        "\n",
        "        for folder, author, midi_file in self.selected_midi_files:\n",
        "            folder = os.path.splitext(os.path.basename(midi_file))[0]\n",
        "            file = os.path.splitext(os.path.basename(midi_file))[0] + \"-1\" # Added only for my files\n",
        "            image_dir = os.path.join(image_root, author, folder)\n",
        "\n",
        "            if os.path.exists(image_dir):\n",
        "                image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg'))]\n",
        "                for file in image_files:\n",
        "                    self.image_paths.append(os.path.join(image_dir, file))\n",
        "\n",
        "        self.image_paths.sort()\n",
        "        print(f\"Selected {len(self.image_paths)} images.\")\n",
        "\n",
        "        if len(self.image_paths) == 0:\n",
        "            raise ValueError(\"No images found for the selected MIDI files. Check directory paths and file structure.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "\n",
        "        rel_path = os.path.relpath(img_path, self.image_root)\n",
        "        composer, piece, _ = rel_path.split(os.sep)\n",
        "        midi_key = f\"{composer}/{piece}\"\n",
        "\n",
        "        image = Image.open(img_path).convert('L')\n",
        "\n",
        "        apply_augmentation = random.randrange(0, 1)\n",
        "        # if apply_augmentation <= self.aug_prob:\n",
        "        #     if self.modify_image:\n",
        "        #         img_array = np.array(image)\n",
        "        #         # angle = random.uniform(-2, 2)\n",
        "        #         angle = 0\n",
        "        #         x_shift = random.randint(-10, 10)\n",
        "        #         y_shift = random.randint(0, 30)\n",
        "\n",
        "        #         rotated_array = modify_image_opencv(img_array, angle, x_shift, y_shift)\n",
        "\n",
        "        #         image = Image.fromarray(rotated_array)\n",
        "\n",
        "        if self.image_transform:\n",
        "            image = self.image_transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "\n",
        "    def modify_image_opencv(image_array, angle, x_shift, y_shift):\n",
        "        (h, w) = image_array.shape[:2]\n",
        "        center = (w // 2, h // 2)\n",
        "\n",
        "        matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "\n",
        "        matrix[0, 2] += x_shift\n",
        "        matrix[1, 2] += y_shift\n",
        "\n",
        "        rotated = cv2.warpAffine(image_array, matrix, (w, h), borderValue=(255, 255, 255))\n",
        "        return rotated"
      ],
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkv9_9e3FfbS"
      },
      "source": [
        "#Uczenie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0TzeGNxwmS0",
        "ExecuteTime": {
          "end_time": "2026-02-16T14:14:41.426172Z",
          "start_time": "2026-02-16T14:14:40.146562Z"
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "def train_epoch(cae, device, dataloader, loss_fn, optimizer):\n",
        "    \"\"\"The training loop of autoencoder.\n",
        "\n",
        "    Args:\n",
        "        cae (classes.resnet_autoencoder.AE): the autoencoder model with - by default- random initilized weights.\n",
        "        device (str): if exists, the accelarator device used from the machine and supported from the pytorch else cpu.\n",
        "        dataloader (DataLoader): loader with the training data.\n",
        "        loss_fn (torch.nn.modules.loss): the loss function of the autoencoder\n",
        "        optimizer (torch.optim): the optimizer of the autoencoder\n",
        "\n",
        "    Returns:\n",
        "        (float): the mean of training loss\n",
        "    \"\"\"\n",
        "    # Set train mode for both the encoder and the decoder\n",
        "    cae.train()\n",
        "    train_loss = []\n",
        "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
        "    for _, (x_batch, y_batch) in tqdm(enumerate(dataloader), total=int(len(dataloader.dataset)/dataloader.batch_size)): # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
        "        # Move tensor to the proper device\n",
        "        x_batch = x_batch.to(device)\n",
        "        # CAE data\n",
        "        decoded_batch,_ = cae(x_batch)\n",
        "        # Evaluate loss\n",
        "        loss = loss_fn(decoded_batch, x_batch)\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Print batch loss\n",
        "        train_loss.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    return np.mean(train_loss)\n",
        "\n",
        "\n",
        "def test_epoch(cae, device, dataloader, loss_fn):\n",
        "    \"\"\"The validation loop of autoencoder on the test dataset.\n",
        "\n",
        "    Args:\n",
        "        cae (classes.resnet_autoencoder.AE): the autoencoder model.\n",
        "        device (str): if exists, the accelarator device used from the machine and supported from the pytorch else cpu.\n",
        "        dataloader (DataLoader): loader with the test data.\n",
        "        loss_fn (torch.nn.modules.loss): the loss function of the autoencoder.\n",
        "\n",
        "    Returns:\n",
        "        (float): the validation loss.\n",
        "    \"\"\"\n",
        "    # Set evaluation mode for encoder and decoder\n",
        "    cae.eval()\n",
        "    with torch.no_grad(): # No need to track the gradients\n",
        "        # Define the lists to store the outputs for each batch\n",
        "        decoded_data = []\n",
        "        original_data = []\n",
        "        for x_batch, _ in dataloader:\n",
        "            # Move tensor to the proper device\n",
        "            x_batch = x_batch.to(device)\n",
        "            # CAE data\n",
        "            decoded_batch,_ = cae(x_batch)\n",
        "            # Append the network output and the original image to the lists\n",
        "            decoded_data.append(decoded_batch.cpu())\n",
        "            original_data.append(x_batch.cpu())\n",
        "        # Create a single tensor with all the values in the lists\n",
        "        decoded_data = torch.cat(decoded_data)\n",
        "        original_data = torch.cat(original_data)\n",
        "        # Evaluate global loss\n",
        "        val_loss = loss_fn(decoded_data, original_data)\n",
        "\n",
        "    return val_loss.data\n",
        "\n",
        "\n",
        "def plot_ae_outputs(cae, dataset_opt, epoch, dataset, device, n=10):\n",
        "    \"\"\"Saving plot diagrams with reconstructed images in comparision with the original ones for a visual assessment.\n",
        "\n",
        "    Args:\n",
        "        cae (classes.resnet_autoencoder.AE): the trained autoencoder model.\n",
        "        dataset_opt (str): the name on the input dataset. Proposed choices ['train_dataset', 'test_dataset']\n",
        "        epoch (int): the present epoch in progress.\n",
        "        device (str): if exists, the accelarator device used from the machine and supported from the pytorch else cpu.\n",
        "        n (int): the number of original images to be plotted with their reconstructions. 10 (default).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(16,4.5))\n",
        "    targets = np.array(dataset.targets)\n",
        "    t_idx = {i:np.where(targets==i)[0][0] for i in range(n)}\n",
        "    for i in range(n):\n",
        "\n",
        "        ax = plt.subplot(2,n,i+1)\n",
        "        img = dataset[t_idx[i]][0] # dataset[t_idx[i]]-> tuple (X,Y)\n",
        "        plt.imshow(img.permute((1, 2, 0))) # rgb\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "        if i == n//2:\n",
        "            ax.set_title('Original images from ' + dataset_opt + ' epoch=' + str(epoch))\n",
        "\n",
        "        ax = plt.subplot(2, n, i + 1 + n)\n",
        "        img = img.unsqueeze(0).to(device) # img -> (3, xx, xx) but img.unsqueeze(0) -> (1,3,xx,xx)\n",
        "        cae.eval()\n",
        "        with torch.no_grad():\n",
        "            rec_img, _  = cae(img)\n",
        "        rec_img = rec_img.cpu().squeeze() # rec_img -> (1, 3, xx, xx) but img.squeeze() -> (3,xx,xx)\n",
        "        plt.imshow(rec_img.permute((1, 2, 0))) # rgb\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "        if i == n//2:\n",
        "            ax.set_title('Reconstructed images from ' + dataset_opt + ' epoch=' + str(epoch))\n",
        "\n",
        "    if not os.path.isdir('output'):\n",
        "        os.mkdir('output')\n",
        "    # plt.show()\n",
        "    plt.savefig(f'output/{epoch}_epoch_from_{dataset_opt}.png')\n",
        "\n",
        "\n",
        "def checkpoint(model, epoch, val_loss, filename):\n",
        "    \"\"\"Saving the model at a specific state.\n",
        "\n",
        "    Args:\n",
        "        model (classes.resnet_autoencoder.AE): the trained autoencoder model.\n",
        "        epoch (int): the present epoch in progress.\n",
        "        val_loss (float): the validation loss.\n",
        "        filename (str): the relative path of the file where the model will be stored.\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            # 'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "            }, filename)\n",
        "\n",
        "def resume(model, filename):\n",
        "    \"\"\"Load the trained autoencoder model.\n",
        "\n",
        "    Args:\n",
        "        model (classes.resnet_autoencoder.AE): the untrained autoencoder model.\n",
        "        filename (str): the relative path of the file where the model is stored.\n",
        "\n",
        "    Results:\n",
        "        model (classes.resnet_autoencoder.AE): the loaded autoencoder model.\n",
        "        epoch (int): the last epoch of the training procedure of the model.\n",
        "        loss (float): the validation loss of the last epoch.\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(filename)\n",
        "    model = model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['val_loss']\n",
        "    return model, epoch, loss"
      ],
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fanelpl-pmc9",
        "ExecuteTime": {
          "end_time": "2026-02-16T14:14:45.428231Z",
          "start_time": "2026-02-16T14:14:45.423285Z"
        }
      },
      "source": [
        "max_seq_len = 96\n",
        "\n",
        "max_midi_files=10240\n",
        "batch_size=32\n",
        "hidden_dim=256\n",
        "rnn_layers=5\n",
        "\n",
        "epochs=50\n",
        "early_stopping = 3\n",
        "\n",
        "learning_rate=0.001\n",
        "weight_decay=0.0001\n",
        "max_norm=1.0\n",
        "\n",
        "MODEL_FILENAME = '/content/drive/MyDrive/modele_ai/autoencoder.ckpt'"
      ],
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNZAypR3wF_6",
        "ExecuteTime": {
          "end_time": "2026-02-16T14:16:28.438550Z",
          "start_time": "2026-02-16T14:16:28.433520Z"
        }
      },
      "source": [
        "# Paths\n",
        "image_root = \"ai_playing_music/src/all_data/generated/my_complex_images/my_midi_images\"\n",
        "midi_root = \"ai_playing_music/src/all_data/generated/generated_complex_midi_processed\"\n",
        "image_root_test = \"ai_playing_music/src/all_data/generated/my_complex_images_test/my_midi_images\"\n",
        "midi_root_test = \"ai_playing_music/src/all_data/generated/generated_complex_midi_processed_test\"\n",
        "\n",
        "selected_image_path = \"/ai_playing_music/src/all_data/generated/my_complex_images/my_midi_images/my_midi_files/song_1/song_1-1.png\"\n",
        "\n",
        "# image_root = \"src/all_data/generated/my_complex_images/my_midi_images\"\n",
        "# midi_root = \"src/all_data/generated/generated_complex_midi_processed\"\n",
        "# image_root_test = \"src/all_data/generated/my_complex_images_test/my_midi_images\"\n",
        "# midi_root_test = \"src/all_data/generated/generated_complex_midi_processed_test\"\n",
        "\n",
        "# selected_image_path = \"src/all_data/generated/my_complex_images/my_midi_images/my_midi_files/song_1/song_1-1.png\""
      ],
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "f-ShNog0Gcib",
        "outputId": "241bd919-2764-4c30-b257-7c052db6e8b5",
        "ExecuteTime": {
          "end_time": "2026-02-16T14:17:48.733339700Z",
          "start_time": "2026-02-16T14:16:31.773327Z"
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((HEIGHT, WIDTH)),\n",
        "    transforms.RandomAffine(degrees=0, shear=2),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "\n",
        "dataset = MusicImageDataset(image_root, midi_root, image_transform, max_midi_files=1024, modify_image=False)\n",
        "val_dataset = MusicImageDataset(image_root_test, midi_root_test, image_transform, max_midi_files=8, modify_image=False)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "val_dataloader = DataLoader(val_dataset, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "cae = AE()\n",
        "epochs = epochs\n",
        "\n",
        "params_to_optimize = [\n",
        "    {'params': cae.parameters()}\n",
        "]\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optim = torch.optim.Adam(params_to_optimize, lr=learning_rate, weight_decay=1e-05)\n",
        "cae.to(device)\n",
        "\n",
        "best_val_loss = 1000000\n",
        "best_epoch = 0\n",
        "#Training loop\n",
        "t1 = datetime.datetime.now()\n",
        "print(\"Start training..\")\n",
        "for epoch in range(epochs):\n",
        "    print('> Epoch ' + str(epoch + 1))\n",
        "    train_loss = train_epoch(cae, device, dataloader, loss_fn, optim)\n",
        "    print(\"Evaluating on test set...\")\n",
        "    val_loss = test_epoch(cae,device, val_dataloader, loss_fn)\n",
        "    print('\\n EPOCH {}/{} \\t train loss {} \\t val loss {}.'.format(epoch + 1, epochs, train_loss, val_loss))\n",
        "    print('Plotting results...')\n",
        "    plot_ae_outputs(cae, \"train_dataset\", epoch, dataset, device,n=10)\n",
        "    plot_ae_outputs(cae, \"test_dataset\", epoch, val_dataset, device,n=10)\n",
        "    if val_loss <= best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            checkpoint(cae, best_epoch, best_val_loss, MODEL_FILENAME)\n",
        "    elif epoch - best_epoch > early_stopping:\n",
        "        print(f\"Early stopped training at epoch {epoch}\")\n",
        "        model, _, _ = resume(cae, MODEL_FILENAME)\n",
        "        break  # terminate the training loop\n",
        "print(\"Total training time:\",datetime.datetime.now()-t1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Selected 1024 images.\n",
            "Selected 8 images.\n",
            "Start training..\n",
            "> Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/32 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipython-input-1720761023.py\", line 67, in __getitem__\n    if apply_augmentation <= self.aug_prob:\n                             ^^^^^^^^^^^^^\nAttributeError: 'MusicImageDataset' object has no attribute 'aug_prob'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1135994021.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> Epoch '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating on test set...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcae\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1501944405.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(cae, device, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Iterate the dataloader (we do not need the label values, this is unsupervised learning)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# with \"_\" we just ignore the labels (the second element of the dataloader tuple)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Move tensor to the proper device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1504\u001b[0m                 \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# be constructed, don't try to instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipython-input-1720761023.py\", line 67, in __getitem__\n    if apply_augmentation <= self.aug_prob:\n                             ^^^^^^^^^^^^^\nAttributeError: 'MusicImageDataset' object has no attribute 'aug_prob'\n"
          ]
        }
      ],
      "execution_count": 19
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}